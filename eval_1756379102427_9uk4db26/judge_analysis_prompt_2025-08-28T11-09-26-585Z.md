# AI Model Evaluation Analysis Prompt

## Your Task
You are an expert AI model evaluator. Analyze the following evaluation results and provide comprehensive insights, recommendations, and comparative analysis.

## Evaluation Overview
- **Date**: 8/28/2025
- **Total Models Evaluated**: 3
- **Successful Models**: 0
- **Overall Success Rate**: 0.0%
- **Evaluation Configuration**: 
  - Timeout: 30s per turn
  - Max Retries: 3
  - Include Online Models: true

## Model Performance Results

### magicoder:7b
- **Status**: FAILED
- **Overall Performance**: Failed to complete
- **Duration**: 107.8s
- **Scenarios Tested**: 3
- **Task Success Rate**: 0.0%
- **Technical Accuracy**: 0.0%
- **Context Understanding**: 0.0%
- **Response Completeness**: 0.0%
- **Domain Knowledge**: 20.0%
- **Code Quality**: 30.0%
- **User Satisfaction**: 0.0%
- **Avg Response Time**: 30010ms

### dolphin-mistral:7b
- **Status**: SKIPPED (Model not available or not responding)
- **Duration**: 35.4s

### codellama:7b
- **Status**: FAILED
- **Overall Performance**: Failed to complete
- **Duration**: 117.9s
- **Scenarios Tested**: 3
- **Task Success Rate**: 0.0%
- **Technical Accuracy**: 0.0%
- **Context Understanding**: 0.0%
- **Response Completeness**: 0.0%
- **Domain Knowledge**: 20.0%
- **Code Quality**: 30.0%
- **User Satisfaction**: 0.0%
- **Avg Response Time**: 30012ms

## Analysis Questions
Please provide detailed analysis for the following:

### 1. Overall Performance Assessment
- Which models performed best and why?
- What patterns do you see in the success/failure rates?
- Are there clear performance tiers among the models?

### 2. Technical Analysis
- Which models showed the strongest technical accuracy?
- How did context understanding vary between models?
- Which models produced the most complete responses?

### 3. Failure Analysis
- What were the common failure modes?
- Which models were skipped and why?
- Are there patterns in timeout/availability issues?

### 4. Domain Expertise Evaluation
- Which models demonstrated the best domain knowledge?
- How did code quality scores compare?
- Which models would be best for specific use cases?

### 5. Performance vs. Efficiency
- Which models provided the best balance of accuracy and speed?
- Are there models that are fast but inaccurate, or slow but thorough?

### 6. Recommendations
- Which models would you recommend for production use?
- What specific use cases would each successful model be best for?
- What improvements could be made to the evaluation process?

## Raw Data
```json
{
  "timestamp": "2025-08-28T11:09:26.562Z",
  "summary": {
    "totalModels": 3,
    "successfulModels": 0,
    "failedModels": 2,
    "skippedModels": 1,
    "successRate": "0.0%"
  },
  "modelResults": [
    {
      "modelId": "ollama:magicoder:7b",
      "modelName": "magicoder:7b",
      "success": false,
      "skipped": false,
      "totalDuration": 107787,
      "retryCount": 0,
      "scenarioCount": 3,
      "overallMetrics": {
        "taskSuccessRate": 0,
        "technicalAccuracy": 0,
        "contextUnderstanding": 0,
        "responseCompleteness": 0,
        "domainKnowledgeScore": 0.20000000000000004,
        "codeQualityScore": 0.3,
        "userSatisfactionScore": 0,
        "responseLatency": 30010
      }
    },
    {
      "modelId": "ollama:dolphin-mistral:7b",
      "modelName": "dolphin-mistral:7b",
      "success": false,
      "skipped": true,
      "skipReason": "Model not available or not responding",
      "totalDuration": 35366,
      "retryCount": 0,
      "scenarioCount": 0
    },
    {
      "modelId": "ollama:codellama:7b",
      "modelName": "codellama:7b",
      "success": false,
      "skipped": false,
      "totalDuration": 117906,
      "retryCount": 0,
      "scenarioCount": 3,
      "overallMetrics": {
        "taskSuccessRate": 0,
        "technicalAccuracy": 0,
        "contextUnderstanding": 0,
        "responseCompleteness": 0,
        "domainKnowledgeScore": 0.20000000000000004,
        "codeQualityScore": 0.3,
        "userSatisfactionScore": 0,
        "responseLatency": 30011.666666666668
      }
    }
  ],
  "configuration": {
    "timeout": 30000,
    "maxRetries": 3,
    "retryDelay": 1000,
    "includeOnlineModels": true,
    "outputDirectory": "c:\\Projects\\apes-foundation\\CompanAI.local.v1",
    "enableLivePreview": true,
    "enableFailsafeMode": true
  }
}
```

---
*Generated by Enhanced Model Evaluation Engine*
*Timestamp: 2025-08-28T11:09:26.585Z*
